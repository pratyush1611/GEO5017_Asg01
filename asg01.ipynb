{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "Implement clustering algorithms\n",
    "1. K-Means\n",
    "1. Heirarchical clustering\n",
    "1. DBSCAN\n",
    "\n",
    "## The Team\n",
    "| Name| Student ID|\n",
    "|------------|---------------|\n",
    "|Cynthia Cai | 5625483 |\n",
    "|Pratyush Kumar | 5359252|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "// add the imports to the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from scipy.spatial import ConvexHull, distance_matrix\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the dataset\n",
    "\n",
    "\n",
    "From the readme for the xyz files, we know that:\n",
    "\n",
    "Ground truth labels:\n",
    "|File range|Label|\n",
    "|--|--|\n",
    "|    000 - 099: |building|\n",
    "|    100 - 199: |car|\n",
    "|    200 - 299: |fence|\n",
    "|    300 - 399: |pole|\n",
    "|    400 - 499: |tree|\n",
    "\n",
    "\n",
    "workflow:\n",
    "\n",
    "iterate through the files, and collect them in a dataframe\n",
    "\n",
    "Use [this link](https://pandas.pydata.org/docs/reference/api/pandas.concat.html#pandas.concat) for concatenating the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyzPath = './scene_objects/data/*.xyz'\n",
    "\n",
    "dataPathsList = glob.glob(xyzPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPointsDF= pd.DataFrame(columns=['x','y','z', 'fileNo', 'groundLabel'])\n",
    "# featureDF = pd.DataFrame(columns=['Label' , 'convHull', median] )\n",
    "\n",
    "def df_maker(df1, df2):\n",
    "    return pd.concat([df1, df2], sort=False, ignore_index=True)\n",
    "\n",
    "labelToGive = None\n",
    "for path in dataPathsList:\n",
    "    indx = int(path.split('/')[-1][0:3])\n",
    "    # if else to determine label\n",
    "    if indx>=0 and indx<100:\n",
    "        labelToGive = 'building' \n",
    "    elif indx>=100 and indx<200:\n",
    "        labelToGive = 'car' \n",
    "    elif indx>=200 and indx<300:\n",
    "        labelToGive = 'fence' \n",
    "    elif indx>=300 and indx<400:\n",
    "        labelToGive = 'pole' \n",
    "    elif indx>=400 and indx<500:\n",
    "        labelToGive = 'tree' \n",
    "\n",
    "    # print(indx, labelToGive)        \n",
    "\n",
    "    # using pandas to read dataset and make a dataFrame\n",
    "    tempDF = pd.read_csv(path, delimiter=' ', header=None, dtype=np.float64, names=['x','y','z'])\n",
    "    tempDF.loc[:,'fileNo'] = indx\n",
    "    tempDF.loc[:,'groundLabel'] = labelToGive\n",
    "\n",
    "    # merge with megaDFofPoints\n",
    "    allPointsDF = df_maker(allPointsDF, tempDF)\n",
    "\n",
    "# allPointsDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pickle file\n",
    "# allPointsDF.to_pickle('./scene_objects/compressedData.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making feature points\n",
    "Identified feature points: `//add more`\n",
    "* median height(z)\n",
    "* convex hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_determiner(indx):\n",
    "    labelToGive=None\n",
    "    if indx>=0 and indx<100:\n",
    "        labelToGive = 'building' \n",
    "    elif indx>=100 and indx<200:\n",
    "        labelToGive = 'car' \n",
    "    elif indx>=200 and indx<300:\n",
    "        labelToGive = 'fence' \n",
    "    elif indx>=300 and indx<400:\n",
    "        labelToGive = 'pole' \n",
    "    elif indx>=400 and indx<500:\n",
    "        labelToGive = 'tree' \n",
    "    return labelToGive\n",
    "\n",
    "\n",
    "featureDF = allPointsDF.groupby('fileNo').var()\n",
    "featureDF.rename(columns={'x':'varX','y':'varY','z':'varZ'}, inplace=True)\n",
    "featureDF.loc[:,'median_Z'] = allPointsDF.groupby('fileNo').z.median()\n",
    "# featureDF.loc[:,'mean_Z'] = allPointsDF.groupby('fileNo').z.mean()\n",
    "\n",
    "# range of x,y,z\n",
    "featureDF.loc[:,'range_X'] = allPointsDF.groupby('fileNo').x.max() - allPointsDF.groupby('fileNo').x.min()\n",
    "featureDF.loc[:,'range_Y'] = allPointsDF.groupby('fileNo').y.max() - allPointsDF.groupby('fileNo').y.min()\n",
    "featureDF.loc[:,'range_Z'] = allPointsDF.groupby('fileNo').z.max() - allPointsDF.groupby('fileNo').z.min()\n",
    "\n",
    "featureDF.loc[:,'Volume'] = allPointsDF.set_index('fileNo').loc[:,'x':'z'].groupby('fileNo').apply(ConvexHull).apply(lambda x: x.volume)\n",
    "\n",
    "# points density\n",
    "featureDF.loc[:,'footprintDensity'] =  allPointsDF.groupby('fileNo').count().x / (featureDF.range_X * featureDF.range_Y)\n",
    "featureDF.loc[:,'volumeDensity'] =  allPointsDF.groupby('fileNo').count().x / featureDF.Volume\n",
    "\n",
    "featureDF.loc[:,'label'] = featureDF.reset_index().fileNo.apply(label_determiner)\n",
    "\n",
    "# standardize DF\n",
    "standardFeatureDF = (featureDF.iloc[:,:-1] - featureDF.iloc[:,:-1].mean() ) / featureDF.iloc[:,:-1].std()\n",
    "\n",
    "# join labels to the feature DF\n",
    "standardFeatureDF = standardFeatureDF.join(other=featureDF.label ,on='fileNo')\n",
    "\n",
    "featureDF.to_pickle('./scene_objects/featureData.pkl')\n",
    "standardFeatureDF.to_pickle('./scene_objects/standardFeatureData.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting to see resemblamces and clusters, if any\n",
    "needed: seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df's\n",
    "featureDF = pd.read_pickle('./scene_objects/featureData.pkl')\n",
    "standardFeatureDF = pd.read_pickle('./scene_objects/standardFeatureData.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=featureDF, hue=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize the feature df </br>\n",
    "[from stackoverflow we see](https://stackoverflow.com/questions/26414913/normalize-columns-of-pandas-data-frame), that we can just use pandas for a standard scaling, or else, a [standard scaler from sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) can also be applied </br>\n",
    "\n",
    "from [answer here](https://stats.stackexchange.com/questions/417339/data-standardization-vs-normalization-for-clustering-analysis), we see that standard scaler is used for k means , so we are going with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=standardFeatureDF, hue=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Algorithms\n",
    "note: already loaded the featureDF and standardised in the cell above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def k_means():\n",
    "    \"\"\"\n",
    "    summary: this function is not yet ready\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heirarchical clustering\n",
    "\n",
    "This [ref was nice](https://www.section.io/engineering-education/hierarchical-clustering-in-python/) for heirarchical clustering understanding\n",
    "Some other sources:\n",
    "* [Statquest](https://www.youtube.com/watch?v=7xHsRkOdVwo&ab_channel=StatQuestwithJoshStarmer)\n",
    "* Penn state [pseudo code](https://online.stat.psu.edu/stat508/lesson/12/12.7)\n",
    "* pseudo code from [researchgate](https://www.researchgate.net/figure/The-hierarchical-clustering-algorithm-in-pseudocode_fig1_202144697)\n",
    "* towards data science article to do [step by step](https://towardsdatascience.com/breaking-down-the-agglomerative-clustering-process-1c367f74c7c2) {this is a good one to follow}\n",
    "* another one [for theory](https://towardsdatascience.com/machine-learning-algorithms-part-12-hierarchical-agglomerative-clustering-example-in-python-1e18e0075019)\n",
    "* similar [theory as above](https://www.geeksforgeeks.org/ml-hierarchical-clustering-agglomerative-and-divisive-clustering/)\n",
    "* real good [step by step explaination](https://medium.com/@darkprogrammerpb/agglomerative-hierarchial-clustering-from-scratch-ec50e14c3826), also the [github code](https://github.com/Darkprogrammerpb/DeepLearningProjects/blob/master/Project40/agglomerative_hierarchial_clustering/Hierarchial%20Agglomerative%20clustering.ipynb)\n",
    "\n",
    "### To Think in heirarchical clustering:\n",
    "* Which type of heirarchical clustering are we doing: lets begin with agglomerative clustering\n",
    "* Within the selected type what distance metrics are we using\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "devise new distance matrix and then repeat the sequence:\n",
    "### TODO: \n",
    "* linkage between the clusters\n",
    "* updation of the distance matrix\n",
    "\n",
    "clusters to be made:\n",
    "`vals.idxmin()` and `idVals.iloc[vals.idxmin()]`\n",
    "\n",
    "* inter cluster distance \n",
    "    * threshold\n",
    "* distance types\n",
    "    * calculate distance yourself\n",
    "* convert to function\n",
    "* check for number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heirarchicalClusterer(standardizedDF = standardFeatureDF, linkageType='complete'):\n",
    "    \"\"\"\n",
    "    Summary:\n",
    "            This function takes in a standardized dataframe with features of objects and returns a list of clusters based on the indices of the dataframe, \n",
    "            note: the distances are calculated on basis of minkowski distance\n",
    "    Arguments:\n",
    "            standardizedDF (pd.DataFrame): standardized dataframe\n",
    "            linkageType (str) : can be 'complete' for complete linkage, 'single' for single linkage and 'average' for average linkage\n",
    "    Return (list) : a list of indices based on the dataframe input, with clusters inside the list, every list will have a length of 2\n",
    "    \"\"\"        \n",
    "    tempDF = standardizedDF.iloc[:,:-1].copy()\n",
    "\n",
    "    distMatDF = pd.DataFrame( distance_matrix(tempDF.values, tempDF.values), index = tempDF.index, columns = tempDF.index)\n",
    "    # replace 0 distances with np.nan\n",
    "    distMatDF = distMatDF.where(distMatDF!=0, np.nan)\n",
    "        \n",
    "    # clustCHECK WILL have two nodes each and a full node\n",
    "    clustCheck = {}\n",
    "    iterationCounter=0\n",
    "    m=len(distMatDF)\n",
    "\n",
    "    while m>1: \n",
    "\n",
    "        # cluster size\n",
    "        # print(f\"Total sample = {m}\")\n",
    "        # compute distances\n",
    "\n",
    "        # get indices with min dist\n",
    "        vals = distMatDF.min(skipna=True)\n",
    "        idVals = distMatDF.idxmin(skipna=True)\n",
    "\n",
    "        # print(vals.min(), vals.idxmin()) # GIVES US THE MINIMUM VALUE and the index at which this was found in the vals series\n",
    "        # print(idVals.iloc[vals.idxmin()])\n",
    "        \n",
    "        ind_to_pop = [idVals.loc[vals.idxmin()] , vals.idxmin()]\n",
    "        # update distmatrix \n",
    "        ## add updated new row, col to dist mat  \n",
    "        ## this updated row is basically the minimum of the two eliminated rows\n",
    "        if linkageType=='complete':\n",
    "            singleLink_minRow = distMatDF.loc[ind_to_pop].drop(ind_to_pop, axis=1).max()\n",
    "        elif linkageType=='single':\n",
    "            singleLink_minRow = distMatDF.loc[ind_to_pop].drop(ind_to_pop, axis=1).min()\n",
    "        elif linkageType=='average':\n",
    "            singleLink_minRow = distMatDF.loc[ind_to_pop].drop(ind_to_pop, axis=1).mean()\n",
    "\n",
    "        singleLink_minRow.rename(f\"cluster {iterationCounter}\", inplace=True)\n",
    "\n",
    "        # pop row and col from dist mat\n",
    "        distMatDF = distMatDF.drop(ind_to_pop, axis=0).drop(ind_to_pop, axis=1)\n",
    "\n",
    "        # min distance from other points\n",
    "        distMatDF = distMatDF.append(singleLink_minRow)\n",
    "        distMatDF.loc[:,singleLink_minRow.name] = singleLink_minRow\n",
    "        # update value of m\n",
    "        m = len(distMatDF)\n",
    "\n",
    "        indPop1, indPop2 = ind_to_pop\n",
    "\n",
    "        clustCheck[f\"cluster {iterationCounter}\"] = {'node1':indPop1 , \"node2\":indPop2, 'fullnodes':ind_to_pop}\n",
    "        # print(\"before\" , clustCheck[f'cluster {iterationCounter}'])\n",
    "        \n",
    "        # Case: if first index is a cluster\n",
    "        if (indPop1 in clustCheck.keys()) and (indPop2 in clustCheck.keys()): #both are clusters\n",
    "            clustCheck[f\"cluster {iterationCounter}\"] = {'node1':clustCheck[indPop1]['fullnodes'].copy() , \"node2\":clustCheck[indPop2]['fullnodes'].copy() }\n",
    "            tempFull = [clustCheck[f\"cluster {iterationCounter}\"][\"node1\"].copy(), clustCheck[f\"cluster {iterationCounter}\"][\"node2\"].copy()]\n",
    "            clustCheck[f\"cluster {iterationCounter}\"][\"fullnodes\"] = tempFull  \n",
    "\n",
    "\n",
    "        # Case: if first index is a cluster\n",
    "        elif indPop1 in clustCheck.keys(): #means first position is cluster\n",
    "            clustCheck[f\"cluster {iterationCounter}\"] = {'node1':clustCheck[indPop1]['fullnodes'].copy() , \"node2\":indPop2 }\n",
    "            tempFull = [clustCheck[f\"cluster {iterationCounter}\"][\"node1\"].copy() , clustCheck[f\"cluster {iterationCounter}\"][\"node2\"]]\n",
    "            clustCheck[f\"cluster {iterationCounter}\"][\"fullnodes\"] = tempFull\n",
    "\n",
    "        # Case: if second index is a cluster\n",
    "        elif indPop2 in clustCheck.keys(): #means first position is cluster\n",
    "            clustCheck[f\"cluster {iterationCounter}\"] = {'node1':indPop1 , \"node2\":clustCheck[indPop2]['fullnodes'].copy()}\n",
    "            tempfull=[clustCheck[f\"cluster {iterationCounter}\"][\"node1\"].copy() , clustCheck[f\"cluster {iterationCounter}\"][\"node2\"]]\n",
    "            clustCheck[f\"cluster {iterationCounter}\"][\"fullnodes\"] =  tempFull\n",
    "\n",
    "        iterationCounter+=1\n",
    "    return clustCheck\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustCheck = heirarchicalClusterer()\n",
    "clustersList = clustCheck[list(clustCheck.keys())[-1]][\"fullnodes\"]\n",
    "\n",
    "\n",
    "def flatten(l):\n",
    "    try:\n",
    "        return flatten(l[0]) + (flatten(l[1:]) if len(l) > 1 else []) if type(l) is list else [l]\n",
    "    except IndexError:\n",
    "        return []\n",
    "\n",
    "# clusters are hardcoded in the following variables\n",
    "cluster1Indices = flatten(clustersList[0][0][0][0])\n",
    "cluster2Indices = flatten(clustersList[0][0][0][1])\n",
    "cluster3Indices = flatten(clustersList[0][0][1])\n",
    "cluster4Indices = flatten(clustersList[0][1])\n",
    "cluster5Indices = flatten(clustersList[1])\n",
    "\n",
    "\n",
    "HeirarchicalClusters = [standardFeatureDF.loc[i] for i in [cluster1Indices,cluster2Indices,cluster3Indices,cluster4Indices,cluster5Indices] ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "* take numbner of output classification in a cluster and check with the actual calues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateModels(classifiedData, originalData):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63416c4ef3d01407afb7ae6f8b52f5ba040582e27e37581275fb0a6850709428"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('simNvis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
