{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "Implement clustering algorithms\n",
    "1. K-Means\n",
    "1. Heirarchical clustering\n",
    "1. DBSCAN\n",
    "\n",
    "## The Team\n",
    "| Name| Student ID|\n",
    "|------------|---------------|\n",
    "|Cynthia Cai | 5625483 |\n",
    "|Pratyush Kumar | 5359252|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "// add the imports to the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from scipy.spatial import ConvexHull, convex_hull_plot_2d\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the dataset\n",
    "\n",
    "\n",
    "From the readme for the xyz files, we know that:\n",
    "\n",
    "Ground truth labels:\n",
    "|File range|Label|\n",
    "|--|--|\n",
    "|    000 - 099: |building|\n",
    "|    100 - 199: |car|\n",
    "|    200 - 299: |fence|\n",
    "|    300 - 399: |pole|\n",
    "|    400 - 499: |tree|\n",
    "\n",
    "\n",
    "\n",
    "workflow:\n",
    "\n",
    "iterate through the files, and collect them in a dataframe\n",
    "\n",
    "Use [this link](https://pandas.pydata.org/docs/reference/api/pandas.concat.html#pandas.concat) for concatenating the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyzPath = './scene_objects/data/*.xyz'\n",
    "\n",
    "dataPathsList = glob.glob(xyzPath)\n",
    "dataPathsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPointsDF= pd.DataFrame(columns=['x','y','z', 'fileNo', 'groundLabel'])\n",
    "# featureDF = pd.DataFrame(columns=['Label' , 'convHull', median] )\n",
    "\n",
    "def df_maker(df1, df2):\n",
    "    return pd.concat([df1, df2], sort=False, ignore_index=True)\n",
    "\n",
    "labelToGive = None\n",
    "for path in dataPathsList:\n",
    "    indx = int(path.split('/')[-1][0:3])\n",
    "    # if else to determine label\n",
    "    if indx>=0 and indx<100:\n",
    "        labelToGive = 'building' \n",
    "    elif indx>=100 and indx<200:\n",
    "        labelToGive = 'car' \n",
    "    elif indx>=200 and indx<300:\n",
    "        labelToGive = 'fence' \n",
    "    elif indx>=300 and indx<400:\n",
    "        labelToGive = 'pole' \n",
    "    elif indx>=400 and indx<500:\n",
    "        labelToGive = 'tree' \n",
    "\n",
    "    # print(indx, labelToGive)        \n",
    "\n",
    "    # using pandas to read dataset and make a dataFrame\n",
    "    tempDF = pd.read_csv(path, delimiter=' ', header=None, dtype=np.float64, names=['x','y','z'])\n",
    "    tempDF.loc[:,'fileNo'] = indx\n",
    "    tempDF.loc[:,'groundLabel'] = labelToGive\n",
    "\n",
    "    # merge with megaDFofPoints\n",
    "    allPointsDF = df_maker(allPointsDF, tempDF)\n",
    "\n",
    "allPointsDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pickle file\n",
    "allPointsDF.to_pickle('./scene_objects/compressedData.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making feature points\n",
    "Identified feature points: `//add more`\n",
    "* median height(z)\n",
    "* convex hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featureDF = pd.DataFrame(columns=['fileNo', 'groundLabel', 'varX', 'varY', 'varZ'])\n",
    "\n",
    "featureDF = allPointsDF.groupby('fileNo').var()\n",
    "featureDF.rename(columns={'x':'varX','y':'varY','z':'varZ'}, inplace=True)\n",
    "featureDF.loc[:,'median_Z'] = allPointsDF.groupby('fileNo').z.median()\n",
    "featureDF.loc[:,'convHull'] = allPointsDF.set_index('fileNo').loc[:,'x':'z'].groupby('fileNo').apply(ConvexHull).apply(lambda x: x.volume)\n",
    "featureDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import megaDF\n",
    "allPointsDF = pd.read_pickle('./scene_objects/compressedData.pkl')\n",
    "allPointsDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means():\n",
    "    \"\"\"\n",
    "    summary: this function is not yet ready\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heirarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63416c4ef3d01407afb7ae6f8b52f5ba040582e27e37581275fb0a6850709428"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('simNvis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
